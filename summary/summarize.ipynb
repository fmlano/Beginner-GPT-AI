{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize\n",
    "\n",
    "Summarize Teams conversations with ChatGPT with VTT files\n",
    "\n",
    "https://webvtt-py.readthedocs.io/en/latest/usage.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import webvtt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['YannMike_2023-03-08.vtt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../vtt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<v Yann>Why is my French accent?</v>\n",
      "<v Yann>I I want to stop doing this experiment where.</v>\n",
      "<v Yann>We some a conversation we record it's generating a VTT file.</v>\n",
      "<v Yann>And I have a a a parcel. I developed a small app in Python that can retrieve the VTT like process it OK.</v>\n",
      "<v Yann>And then.</v>\n",
      "<v Yann>I want to feed it through the to GPT stuff API and the API is amazing. I don't know if you've played around with it already.</v>\n",
      "<v Mike>No, I need to though the. Yeah, the problem is I can't find too many things. I promised people so every day.</v>\n"
     ]
    }
   ],
   "source": [
    "file = 'YannMike_2023-03-08.vtt'\n",
    "chat = webvtt.read('../vtt/'+file)\n",
    "for caption in chat[0:10]:\n",
    "    # print(f'From {caption.start} to {caption.end}')\n",
    "    print(caption.raw_text)\n",
    "    # print(caption.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'YannMike_2023-03-08.txt'\n",
    "# for caption in chat:\n",
    "#     with open('txt/'+txt,mode='a') as f:\n",
    "#         f.write(caption.text+'\\n')\n",
    "\n",
    "str = []\n",
    "for caption in chat:\n",
    "    str.append(caption.text)\n",
    "sep = '\\n'\n",
    "convo = sep.join(str)\n",
    "with open('../txt/'+txt,mode='w') as f:\n",
    "    f.write(convo)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many Tokens?\n",
    "- https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n",
    "- https://github.com/openai/tiktoken\n",
    "- https://platform.openai.com/tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first order\n",
    "tokens = convo.split() # split the string into tokens\n",
    "num_tokens = len(tokens)\n",
    "num_tokens\n",
    "\n",
    "# # pip install nltk\n",
    "# import nltk\n",
    "# len(nltk.word_tokenize(convo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "encoding_name = 'cl100k_base'\n",
    "encoding = tiktoken.get_encoding(encoding_name)\n",
    "num_tokens = len(encoding.encode(convo))\n",
    "num_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tomli\n",
    "import openai\n",
    "with open('../.streamlit/secrets.toml','rb') as f:\n",
    "    toml_dict = tomli.load(f)\n",
    "openai.api_key = toml_dict['OPEN_AI_KEY']\n",
    "os.environ['OPENAI_API_KEY'] = toml_dict['OPEN_AI_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The conversation appears to be about using an app in Python to retrieve a VTT file generated from a recorded conversation, and then feeding it through the GPT stuff API. The speaker mentions their French accent and expresses difficulty in finding information on the topic. The second person expresses interest in the API and mentions needing to try it out.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = 'summarize the following conversation'\n",
    "model = 'gpt-3.5-turbo'\n",
    "# model = 'gpt-4'\n",
    "completion = openai.ChatCompletion.create(\n",
    "    model=model,\n",
    "      messages=[\n",
    "        {'role': 'system','content': context},\n",
    "        {'role': 'user', 'content': convo}\n",
    "            ]\n",
    ")\n",
    "completion.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAI(cache=None, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x000001C723A661D0>, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.7, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key=None, batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI()\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Q: What did the fish say when it hit the wall?\n",
      "A: Dam!\n"
     ]
    }
   ],
   "source": [
    "print(llm('tell me a joke'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x000001C723A661D0>, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.7, model_kwargs={}, openai_api_key=None, request_timeout=60, max_retries=6, streaming=False, n=1, max_tokens=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "chat = ChatOpenAI(model_name='gpt-3.5-turbo')\n",
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "\n",
    "loader = UnstructuredFileLoader(\"../txt/YannMike_2023-03-08.txt\")\n",
    "doc = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why is my French accent?\\n\\nI I want to stop doing this experiment where.\\n\\nWe some a conversation we record it's generating a VTT file.\\n\\nAnd I have a a a parcel. I developed a small app in Python that can retrieve the VTT like process it OK.\\n\\nAnd then.\\n\\nI want to feed it through the to GPT stuff API and the API is amazing. I don't know if you've played around with it already.\\n\\nNo, I need to though the. Yeah, the problem is I can't find too many things. I promised people so every day.\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0].dict()['page_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Why is my French accent?\n",
      "\n",
      "I I want to stop doing this experiment where.\n",
      "\n",
      "We some a conversation we record it's generating a VTT file.\n",
      "\n",
      "And I have a a a parcel. I developed a small app in Python that can retrieve the VTT like process it OK.\n",
      "\n",
      "And then.\n",
      "\n",
      "I want to feed it through the to GPT stuff API and the API is amazing. I don't know if you've played around with it already.\n",
      "\n",
      "No, I need to though the. Yeah, the problem is I can't find too many things. I promised people so every day.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The speaker is discussing an experiment where they record conversations and generate a VTT file, which they then retrieve and process using a Python app they developed. They plan to feed it through the GPT API, but are struggling to find enough information about it. They mention having a French accent, but do not elaborate further.'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "chain = load_summarize_chain(chat,\n",
    "                             chain_type='stuff',\n",
    "                             verbose=True,\n",
    "                             )\n",
    "chain.run(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 3000,\n",
    "    chunk_overlap = 200\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"../txt/chat.txt\")\n",
    "doc = loader.load()\n",
    "docs = text_splitter.split_documents(doc)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(chat,\n",
    "                             chain_type='map_reduce',\n",
    "                            #  verbose=True,\n",
    "                             )\n",
    "chain.run(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78ea38d917773e9a07c088416f37ab1e4e4409d94e11cb7b8d387de788dd2d7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
